#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YouTube-Ğ¿Ğ°Ñ€ÑĞµÑ€: Shorts + Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¸, Ğ±ĞµĞ· live/upcoming.
Ğ¡Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° (col A) Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ URL (col J) Ğ¸Ğ· Google Sheets Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² Ğ»Ğ¸ÑÑ‚ "Results".
ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµÑ‚ Ñ Ğ¼ĞµÑÑ‚Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· shelve.
ĞŸÑ€Ğ¸ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ²Ğ¾Ñ‚Ñ‹ Ğ²ÑĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸ Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ¶Ğµ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.
Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ BATCH_SIZE ĞºĞ»ÑÑ‡ĞµĞ¹.
"""
import os
import sys
import shelve
import time
import logging
import socket
from pathlib import Path
from datetime import datetime, timezone

import pandas as pd
import isodate
from langdetect import detect, LangDetectException
from dotenv import load_dotenv, dotenv_values
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.oauth2 import service_account

# â”€â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# â”€â”€â”€ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (Ğ¸Ğ· .env) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¯Ğ²Ğ½Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ÑƒÑ‚ÑŒ Ğº .env Ñ€ÑĞ´Ğ¾Ğ¼ ÑĞ¾ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
env_path = Path(__file__).parent / ".env"
if not env_path.exists():
    logger.error(f"âŒ .env Ñ„Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ¿Ğ¾ Ğ¿ÑƒÑ‚Ğ¸ {env_path}")
    sys.exit(1)

load_dotenv(dotenv_path=env_path, override=True)
SHEET_ID = os.getenv("SHEET_ID")
logger.info(f"ğŸš© Loaded SHEET_ID from {env_path}: {SHEET_ID}")

KEYWORDS_SHEET       = os.getenv("KEYWORDS_SHEET", "Keywords")
RESULTS_SHEET        = os.getenv("RESULTS_SHEET", "Results")
SERVICE_ACCOUNT_JSON = os.getenv("SERVICE_ACCOUNT_JSON")
NUM_RESULTS          = int(os.getenv("NUM_RESULTS", 10))
REGION               = os.getenv("REGION", "US")
MAX_PAGES            = int(os.getenv("MAX_PAGES", 1))
OUTPUT_CSV           = os.getenv("OUTPUT_CSV", "yt_results.csv")
BATCH_SIZE           = int(os.getenv("BATCH_SIZE", 50))

if MAX_PAGES < 1:
    sys.exit("âŒ MAX_PAGES Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ â‰¥ 1")

# â”€â”€â”€ Google Sheets API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
creds_sheets = service_account.Credentials.from_service_account_file(
    SERVICE_ACCOUNT_JSON, scopes=SCOPES
)
sheets_service = build(
    'sheets', 'v4',
    credentials=creds_sheets,
    cache_discovery=False
)

# â”€â”€â”€ Ğ¥Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ ĞºĞµÑˆĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CACHE = shelve.open("yt_cache.db")

# â”€â”€â”€ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ½Ğ¸Ñ ĞºĞ²Ğ¾Ñ‚Ñ‹ Ğ²ÑĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class QuotaExceededAllKeys(Exception):
    """Ğ’ÑĞµ ĞºĞ»ÑÑ‡Ğ¸ YouTube API Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ»Ğ¸ ĞºĞ²Ğ¾Ñ‚Ñƒ"""
    pass

# â”€â”€â”€ ĞšĞ»Ğ°ÑÑÑ‹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ API-ĞºĞ»ÑÑ‡Ğ°Ğ¼Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class APIKey:
    def __init__(self, key: str):
        self.key = key
        self.service = build(
            "youtube", "v3",
            developerKey=key,
            cache_discovery=False
        )
        self.used_units = 0
        self.active = True

class KeyManager:
    def __init__(self, keys: list[str]):
        self.keys = [APIKey(k) for k in keys]
        self.index = 0

    def get_key(self) -> APIKey:
        n = len(self.keys)
        for _ in range(n):
            api = self.keys[self.index]
            self.index = (self.index + 1) % n
            if api.active:
                return api
        raise QuotaExceededAllKeys()

    def deactivate(self, api: APIKey):
        api.active = False
        logger.warning(f"Ğ”ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ ĞºĞ»ÑÑ‡: {api.key}")

    def record(self, api: APIKey, units: int):
        api.used_units += units
        logger.info(f"ĞšĞ»ÑÑ‡ {api.key}: Ñ€Ğ°ÑÑ…Ğ¾Ğ´ {units} units (Ğ¸Ñ‚Ğ¾Ğ³Ğ¾ {api.used_units})")

    def execute(self, fn, units=0, backoff_max=3):
        attempt = 0
        while True:
            api = self.get_key()
            try:
                resp = fn(api.service).execute()
                used = units(resp) if callable(units) else units
                self.record(api, used)
                return resp
            except HttpError as e:
                err_str = str(e)
                if 'quotaExceeded' in err_str or 'dailyLimitExceeded' in err_str:
                    self.deactivate(api)
                    continue
                if 'rateLimitExceeded' in err_str:
                    if attempt < backoff_max:
                        delay = 2 ** attempt
                        logger.warning(
                            f"Rate limitExceeded Ğ½Ğ° ĞºĞ»ÑÑ‡Ğµ {api.key}, Ğ¶Ğ´Ñƒ {delay}s (Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {attempt+1})"
                        )
                        time.sleep(delay)
                        attempt += 1
                        continue
                    else:
                        self.deactivate(api)
                        continue
                logger.error(f"Unexpected HttpError Ğ½Ğ° ĞºĞ»ÑÑ‡Ğµ {api.key}: {e}")
                raise
            except (ConnectionResetError, socket.error) as e:
                if attempt < backoff_max:
                    delay = 2 ** attempt
                    logger.warning(
                        f"Connection error Ğ½Ğ° ĞºĞ»ÑÑ‡Ğµ {api.key}: {e}, Ğ¶Ğ´Ñƒ {delay}s (Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {attempt+1})"
                    )
                    time.sleep(delay)
                    attempt += 1
                    continue
                else:
                    self.deactivate(api)
                    continue

# â”€â”€â”€ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ĞºĞ»ÑÑ‡ĞµĞ¹ YouTube API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
raw_keys = os.getenv("YT_API_KEYS") or dotenv_values(env_path).get("YT_API_KEYS", "")
KEYS = [k.strip() for k in raw_keys.split(",") if k.strip()]
if not KEYS:
    sys.exit("âŒ ĞĞµÑ‚ API-ĞºĞ»ÑÑ‡ĞµĞ¹ (YT_API_KEYS).")
logger.info(f"ğŸ”‘ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(KEYS)} ĞºĞ»ÑÑ‡ĞµĞ¹")
key_manager = KeyManager(KEYS)

VIDEO_PARTS = "snippet,contentDetails,status,player,statistics"

# â”€â”€â”€ Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def iso2hms(iso: str):
    if not iso:
        return "", 0
    td = isodate.parse_duration(iso)
    s = int(td.total_seconds())
    h, r = divmod(s, 3600)
    m, sec = divmod(r, 60)
    return f"{h:02d}:{m:02d}:{sec:02d}", s

def fmt_age(pub: str):
    if not pub:
        return ""
    dt = datetime.fromisoformat(pub.replace('Z', '+00:00'))
    d = (datetime.now(timezone.utc) - dt).days
    y, rem = divmod(d, 365)
    mo = rem // 30
    return f"{y} years {mo} months" if y else f"{mo} months"

def safe_detect(txt: str):
    try:
        return detect(txt) if txt.strip() else ""
    except LangDetectException:
        return ""

# â”€â”€â”€ Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Google Sheets Ğ¸ CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def append_to_sheets_with_retry(service, spreadsheet_id, sheet_range, values,
                                max_retries=5, base_delay=1):
    for attempt in range(1, max_retries+1):
        try:
            service.spreadsheets().values().append(
                spreadsheetId=spreadsheet_id,
                range=sheet_range,
                valueInputOption='RAW',
                body={'values': values}
            ).execute()
            logger.info("âœ… Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ»Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Google Sheets")
            return True
        except (HttpError, ConnectionResetError, socket.error) as e:
            delay = base_delay * 2**(attempt-1)
            logger.warning(
                f"âš ï¸ ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {attempt}/{max_retries} Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Sheets ÑƒĞ¿Ğ°Ğ»Ğ°: {e}. Ğ–Ğ´Ñ‘Ğ¼ {delay}s."
            )
            time.sleep(delay)
    logger.error("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ² Google Sheets Ğ¿Ğ¾ÑĞ»Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
    return False

def save_csv_with_retry(df: pd.DataFrame, path: str, max_retries=3, base_delay=1):
    for attempt in range(1, max_retries+1):
        try:
            df.to_csv(path, index=False)
            logger.info(f"âœ… CSV ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {path}")
            return True
        except Exception as e:
            delay = base_delay * 2**(attempt-1)
            logger.warning(
                f"âš ï¸ ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {attempt}/{max_retries} ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ CSV ÑƒĞ¿Ğ°Ğ»Ğ°: {e}. Ğ–Ğ´Ñ‘Ğ¼ {delay}s."
            )
            time.sleep(delay)
    logger.error("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ CSV Ğ¿Ğ¾ÑĞ»Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
    return False

def append_csv_batch(df: pd.DataFrame, path: str, max_retries=3, base_delay=1) -> bool:
    """Ğ”Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ DataFrame Ğ² CSV, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ„Ğ°Ğ¹Ğ» Ñ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¼, ĞµÑĞ»Ğ¸ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚."""
    p = Path(path)
    header = not p.exists()
    for attempt in range(1, max_retries+1):
        try:
            df.to_csv(path, index=False, header=header, mode='a')
            logger.info(f"âœ… CSV Ğ±Ğ°Ñ‚Ñ‡ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ ({len(df)} ÑÑ‚Ñ€Ğ¾Ğº): {path}")
            return True
        except Exception as e:
            delay = base_delay * 2**(attempt-1)
            logger.warning(f"âš ï¸ ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {attempt}/{max_retries} Ğ´Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ CSV ÑƒĞ¿Ğ°Ğ»Ğ°: {e}. Ğ–Ğ´Ñ‘Ğ¼ {delay}s.")
            time.sleep(delay)
    logger.error("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ´Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ CSV Ğ¿Ğ¾ÑĞ»Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
    return False

# â”€â”€â”€ ĞŸĞ¾Ğ¸ÑĞº YouTube Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡Ñƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_once(keyword: str, input_url: str) -> list:
    rows, token, page = [], None, 0
    while len(rows) < NUM_RESULTS and page < MAX_PAGES:
        page += 1
        sk = f"S:{keyword}:{REGION}:{token or ''}"
        if sk in CACHE:
            sr = CACHE[sk]
        else:
            sr = key_manager.execute(
                lambda svc: svc.search().list(
                    q=keyword,
                    part="id",
                    type="video",
                    order="relevance",
                    regionCode=REGION,
                    maxResults=min(50, NUM_RESULTS - len(rows)),
                    pageToken=token or "",
                    safeSearch="none"
                ),
                units=100
            )
            CACHE[sk] = sr
        vids = [it.get('id', {}).get('videoId') for it in sr.get('items', []) if it.get('id', {}).get('videoId')]
        if not vids:
            break
        vk = f"V:{','.join(vids)}"
        if vk in CACHE:
            vr = CACHE[vk]
        else:
            vr = key_manager.execute(
                lambda svc: svc.videos().list(
                    id=",".join(vids),
                    part=VIDEO_PARTS
                ),
                units=lambda resp: len(resp.get('items', []))
            )
            CACHE[vk] = vr
        cids = list({it['snippet']['channelId'] for it in vr.get('items', [])})
        amap = {}
        if cids:
            ck = f"C:{','.join(cids)}"
            if ck in CACHE:
                ch = CACHE[ck]
            else:
                ch = key_manager.execute(
                    lambda svc: svc.channels().list(
                        part="snippet",
                        id=",".join(cids)
                    ),
                    units=1
                )
                CACHE[ck] = ch
            amap = {c['id']: c['snippet']['thumbnails']['default']['url'] for c in ch.get('items', [])}
        for it in vr.get('items', []):
            sn = it.get('snippet', {})
            if sn.get('liveBroadcastContent') in {'live', 'upcoming'}:
                continue
            det = it.get('contentDetails', {})
            dur_str, dur_s = iso2hms(det.get('duration', ''))
            st = it.get('status', {})
            stats = it.get('statistics', {})
            author = sn.get('channelTitle', '')
            avatar = amap.get(sn.get('channelId', ''), '')
            if sn.get('defaultAudioLanguage'):
                ls, lang = 'audio', sn['defaultAudioLanguage']
            elif sn.get('defaultLanguage'):
                ls, lang = 'default', sn['defaultLanguage']
            else:
                ls, lang = 'detect', safe_detect(sn.get('title', '') + ' ' + sn.get('description', '')) or 'unknown'
            lic = st.get('license', '')
            emb = st.get('embeddable', False)
            allowed = emb and lic == 'creativeCommon'
            rows.append([
                keyword, input_url, it['id'], sn.get('title', ''), sn.get('description', ''),
                lang, ls, 'short' if dur_s < 60 else 'video',
                dur_str, dur_s, fmt_age(sn.get('publishedAt', '')), author, avatar,
                stats.get('viewCount', ''), stats.get('likeCount', ''), stats.get('dislikeCount', ''),
                emb, lic, allowed,
                it.get('player', {}).get('embedHtml', f"<iframe src=\"https://www.youtube.com/embed/{it['id']}\" allowfullscreen></iframe>")
            ])
        token = sr.get('nextPageToken') or None
    return rows

# â”€â”€â”€ Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
r1 = sheets_service.spreadsheets().values().get(
    spreadsheetId=SHEET_ID,
    range=f"{KEYWORDS_SHEET}!A2:A"
).execute()
r2 = sheets_service.spreadsheets().values().get(
    spreadsheetId=SHEET_ID,
    range=f"{KEYWORDS_SHEET}!J2:J"
).execute()
keywords = [r[0] for r in r1.get('values', [])]
input_urls = [r[0] for r in r2.get('values', [])]

prog = shelve.open('progress.db')
start = prog.get('last_index', 0)
all_res = []
batch_res = []

try:
    for idx in range(start, len(keywords)):
        kw = keywords[idx]
        input_url = input_urls[idx] if idx < len(input_urls) else ''
        logger.info(f"ğŸ” [{idx}] {kw}")
        res = search_once(kw, input_url)

        all_res.extend(res)
        batch_res.extend(res)
        prog['last_index'] = idx + 1

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ‚Ñ‡Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ BATCH_SIZE ĞºĞ»ÑÑ‡ĞµĞ¹
        if (idx + 1 - start) % BATCH_SIZE == 0:
            df_batch = pd.DataFrame(batch_res, columns=[
                'keyword','input_url','videoId','title','description','language','language_source',
                'video_type','duration','duration_seconds','age','author','author_avatar',
                'view_count','like_count','dislike_count','embeddable','license',
                'allowed_on_third_party','iframe'
            ])
            append_csv_batch(df_batch, OUTPUT_CSV)
            append_to_sheets_with_retry(
                sheets_service,
                SHEET_ID,
                f"{RESULTS_SHEET}!A2",
                batch_res
            )
            logger.info(f"ğŸ”„ Ğ‘Ğ°Ñ‚Ñ‡ Ğ¸Ğ· {len(batch_res)} Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ Ğ¿Ğ¾ÑĞ»Ğµ {(idx+1-start)} ĞºĞ»ÑÑ‡ĞµĞ¹")
            batch_res = []

except QuotaExceededAllKeys:
    logger.warning("âš ï¸ Ğ’ÑĞµ ĞºĞ»ÑÑ‡Ğ¸ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ»Ğ¸ ĞºĞ²Ğ¾Ñ‚Ñƒ, Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ")
finally:
    prog.close()
    CACHE.close()

# Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ
if batch_res:
    df_batch = pd.DataFrame(batch_res, columns=[
        'keyword','input_url','videoId','title','description','language','language_source',
        'video_type','duration','duration_seconds','age','author','author_avatar',
        'view_count','like_count','dislike_count','embeddable','license',
        'allowed_on_third_party','iframe'
    ])
    append_csv_batch(df_batch, OUTPUT_CSV)
    append_to_sheets_with_retry(
        sheets_service,
        SHEET_ID,
        f"{RESULTS_SHEET}!A2",
        batch_res
    )
    logger.info(f"ğŸ”„ Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ñ‚Ñ‡ Ğ¸Ğ· {len(batch_res)} ÑÑ‚Ñ€Ğ¾Ğº ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½")

# Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ°ÑÑ…Ğ¾Ğ´ ĞºĞ²Ğ¾Ñ‚Ñ‹
total_units = sum(api.used_units for api in key_manager.keys)
logger.info(f"â„¹ï¸ Ğ’ÑĞµĞ³Ğ¾ Ñ€Ğ°ÑÑ…Ğ¾Ğ´: {total_units} units")
